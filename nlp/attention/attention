Tradiional Solution (Pre 2018)
Back in the day, NLP used to use seq2seq models. Or encoder-encoder models with RNN units with LSTms

Encoder:
RNN takes input vectors in one language and then take final hidden states as a "context vectors"

Decoder:
Another RNN takes input the previous words and generates the next work. The important part is the initial state of the RNN is the context vector of the encoder

Issue: The BLUE score went down the sence length increases
This starts to fail as the sequence gets longer and longer in the context vector, can't remember all long text and creates a informational bottleneck


Today:
Cross Attention: Given two sentences:
1 = "I am sure he is NICE" 
2 =  "I work at NICE" 

We take the word like "NICE", its meaning will change with respect with the other words in the sentence. Sentence 1 refers to the personality trait of NICE, while 2 has a strong relationship with coding and a software company

![Alt text](image.png)

